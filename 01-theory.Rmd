# Theoretical Background

## Frequentist Methods

In frequentist methods, the **estimate** is the value of the parameter ($\theta$) which if it was true would be most likely to give rise to the observed data [@millar_maximum_2011].
In other words, frequentist methods aim to select the value of $\theta$ which maximizes the likelihood

$$P(D|\theta X)$$

where $P(D|\theta X)$ is the probability of the observed data ($D$) given the parameter value ($\theta$) and background information ($X$).

The **95% confidence interval** is the range of values of $\theta$ which if they were true would have a 5% or greater probability of giving rise to data with a parameter estimate no more extreme than that observed.

The **p-value** is the probability that if the true parameter value was 0 (no effect) that it would give rise to data with a parameter estimate no more extreme than that observed.

By convention a p-value \< 0.05 is statistically **significant** and this is equivalent to a 95% confidence interval that does not span 0 (no effect).


```{r sig, echo = FALSE, fig.height = 3, fig.width = 6, fig.cap = "Example 95% confidence intervals."}
library(ggplot2)
large <- data.frame(y = 0, ymin = -0.2, ymax = 0.2) 
small <- data.frame(y = 0, ymin = -0.05, ymax = 0.05)

data <- rbind(large, small, large - 0.06, small - 0.06, 
                  large - 0.225, small - 0.225)

data$y[1] <- -0.15
data$ymin[1] <- -0.5
data$ymax[1] <- 0.2

data$Significance <- factor("Not Significant", levels = c("Not Significant", "Significant"))
data$Significance[data$ymax < 0] <- "Significant"

data$x <- 1:nrow(data)

gp_significance <- ggplot(data = data, aes(x = x, y = y)) +
  geom_pointrange(aes(ymin = ymin, ymax = ymax, color = Significance)) +
  geom_hline(yintercept = 0, linetype = "dotted") +
  scale_y_continuous("Effect (%)", labels = percent) + 
  scale_color_manual(values = c("black", "blue")) + 
  scale_x_continuous(breaks = 1:6) +
  ylab("Estimate") +
  theme(axis.title.y = element_blank()) +
  NULL

gp_significance
```



If we are willing to assume that we know the direction of change then the p-value is halved.

The **power** is the probability that a p-value will be *significant* for a particular parameter value and sample size.

As is always the case the values are conditional on background information $X$.

## Bayesian Methods

Bayesian methods take the probability of each parameter value giving rise to the observed data if it was true and multiply it by the probability of it being true (given the background information) to get the probability of each parameter value being true given the observed data.

$$P(\theta|DX)  \propto P(D|\theta X) \cdot P(\theta|X)$$

The resultant probability distribution provides a complete representation of the uncertainty although it can be summarized in terms of an estimate (often median), a 95% credible interval (typically 95% quantiles) which has a 95% probability of including the true value (conditional on the background information) and a p-value which is the probability of the true effect being in the opposite direction to the median. 

Statistical significance and power are as defined above for frequentist methods.

### The Logic of Science

Bayesian methods represent the extension of formal logic from certain to uncertain situations @jaynes_probability_2003.

For example formal logic can take the premise

> All sharks are fish.

and a second premise

> x is not a fish.

and conclude with certainty that 'x is not a shark'.

Bayesian inference on the other hand can take a premise such as

> Most sharks are fish.

and a second premise

> x is probably not a fish

and estimate with uncertainty the probability that x is not a shark.

## Frequentist vs Bayesian

Standard frequentist methods typically use fast heuristics but their validity depends on the sample size.

Standard Bayesian methods can be slower to fit but do not depend on the sample size; can readily handle missing values; and allow the uncertainty in derived parameters to be quantified.

Frequentist analyses can, and generally are, interpreted as Bayesian analyses with uninformative priors [@jaynes_probability_2003; @mcelreath_statistical_2020]. 
If prior information is available then its incorporation leads to more reliable estimates [@jaynes_probability_2003; @mcelreath_statistical_2020], particularly with small sample sizes.

In this sense frequentist methods can be viewed as a form of Bayesian heuristic [@jaynes_probability_2003].

## P-values

In 2016 the American Statistical Association [@wasserstein_asas_2016] provided a statement on p-values. 
It included the following

**A p-value, or statistical significance, does not measure the size of an effect or the importance of a result.**

> Smaller p-values do not necessarily imply the presence of larger or more important effects, and larger p-values do not imply a lack of importance or even lack of effect. 
Any effect, no matter how tiny, can produce a small p-value if the sample size or measurement precision is high enough, and large effects may produce unimpressive p-values if the sample size is small or measurements are imprecise.

In other words, p-values confound the estimated size of the effect with the uncertainty (width of the confidence interval) which is what they were intended to do [@greenland2019]. 
The solution is to also report the estimate and associated confidence/credible intervals; an approach sometimes referred to as effect size estimation/reporting.

**Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.**

> Practices that reduce data analysis or scientific inference to mechanical 'bright-line' rules (such as 'p \< 0.05') for justifying scientific claims or conclusions can lead to erroneous beliefs and poor decision making...Pragmatic considerations often require binary, 'yes-no' decisions, but this does not mean that p-values alone can ensure that a decision is correct or incorrect.

Important decisions should be based on decision analysis, ie the estimated consequences (with uncertainty) of alternative actions on human values [@hemming_introduction_2022].
And data collection, which is itself a form of decision, should be based on the probable benefits of improving a decision versus the cost of collecting the data [@canessa2015]. 

However, less consequential data collections decisions such as how many salmonids to sample or tag can be based on simpler heuristics such as the expected power.

## Power Analysis

Power analysis can be used to determine how much data to collect to have a reasonable probability (typically 0.8) of a statistically significant result (typically p-value \< 0.05) for a given effect size.

It is worth noting that once a sample has been collected a post-hoc power analysis provides no information beyond that already provided by the estimated effect size with uncertainty [@colegrave_confidence_2003].

To understand the inter-dependence among effect size and power consider that a p-value of < 0.05 corresponds to a effect size with a 95% confidence interval that doesn't span 0.

In the case of very simple models, heuristic equations have been developed to allow the power to be quickly calculated.
More complex models require hundreds of data sets to be simulated and analysed which depending on the complexity of the model can require a lot of computational time.

## Summary

In short the use of frequentist power analyses to determine the number of salmon to tag and scale can be viewed and justified as a quick Bayesian Decision Analytic heuristic.

However, it is important to have a general understanding of the theoretical background so that if frequentist power analysis proves inadequate for a particular problem it is clear how to alter it to make an informed decision. For more information see McElreath [@mcelreath_statistical_2020].
