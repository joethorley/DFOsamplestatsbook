--- 
title: "Sample Size and Power for Salmon Sampling and Marking"
author: "Joe Thorley"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
url: https://poisson-dfosamplestatsbook.netlify.app
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This booklet was prepared for the Department of Fisheries and Ocean's 
  Salmonid Enhancement Program in Vancouver for a one day course on
  sample size and power for salmon sampling and marking.
link-citations: yes
github-repo: rstudio/bookdown-demo
---

# Overview {-}

This course was developed at the request of Michael Thom who stated that

> We have a fairly small team and are looking for a one-day crash course in sample size calculation methods and considerations (power analysis). 
> We all have a basic understanding of stats and how to program in R, so the team would be able to jump right into this focused topic.

As requested the course focuses on frequentist methods and power analysis although it begins with a broader introduction to Bayesian methods and effect sizes.

## Materials {-}

This booklet is available at

<https://poisson-dfosamplestatsbook.netlify.app>

password *salmonids*.

There is a currently private R data package associated with the book which is available at

<https://github.com/poissonconsulting/DFOsalmonids>

and a currently private R package of simple analytic functions at

<https://github.com/poissonconsulting/DFOsamplestats>

although the website describing the functions is available at <https://poissonconsulting.github.io/DFOsamplestats/>.

Following the course, a hosting fee will be charged after six months unless the book and the R packages are transferred to the [DFO github](https://github.com/dfo-mpo) site.

## Users {-}

This booklet was written to be a teaching guide for a one day crash course (taught as two half days).
As such the booklet will be most useful for course participants both during and after the course.

The booklet is not expected to be useful to researchers who have not taken the course especially if they little understanding of statistics and/or R.

<!--chapter:end:index.Rmd-->

# Theoretical Background

## Frequentist Methods

In frequentist methods, the **estimate** is the value of the parameter ($\theta$) which if it was true would be most likely to give rise to the observed data [@millar_maximum_2011].
In other words, frequentist methods aim to select the value of $\theta$ which maximizes the likelihood

$$P(D|\theta X)$$

where $P(D|\theta X)$ is the probability of the observed data ($D$) given the parameter value ($\theta$) and background information ($X$).

The **95% confidence interval** is the range of values of $\theta$ which if they were true would have a 5% or greater probability of giving rise to data with a parameter estimate no more extreme than that observed.

The **p-value** is the probability that if the true parameter value was 0 (no effect) that it would give rise to data with a parameter estimate no more extreme than that observed.

By convention a p-value \< 0.05 is statistically **significant** and this is equivalent to a 95% confidence interval that does not span 0 (no effect).

If we are willing to assume that we know the direction of change then the p-value is halved.

As is always the case the values are conditional on background information $X$.

## Bayesian Methods

Bayesian methods take the probability of each parameter value giving rise to the observed data if it was true and multiply it by the probability of it being true (given the background information) to get the probability of each parameter value being true given the observed data.

$$P(\theta|DX)  \propto P(D|\theta X) \cdot P(\theta|X)$$

The resultant probability distribution provides a complete representation of the uncertainty although it can be summarized in terms of an estimate (often median), a 95% credible interval (typically 95% quantiles) which has a 95% probability of including the true value (conditional on the background information) and a p-value which is the probability of the true effect being in the opposite direction to the median. 

### The Logic of Science

Bayesian methods represent the extension of formal logic from certain to uncertain situations @jaynes_probability_2003.

For example formal logic can take the premise

> All sharks are fish.

and a second premise

> x is not a fish.

and conclude with certainty that x is not a shark.

Bayesian inference on the other hand can take a premise such as

> Most sharks are fish.

and a second premise

> x is probably not a fish

and estimate with uncertainty the probability that x is not a shark.

## Heuristics

Heuristics are quicker but less reliable methods to arrive at an answer.

Standard frequentist methods typically use fast heuristics but their validity depends on the sample size.
Standard Bayesian methods can be slower to fit but do not depend on the sample size; can readily handle missing values; and allow the uncertainty in derived parameters to be quantified.

Frequentist analyses can, and generally are, interpreted as Bayesian analyses with uninformative priors [@jaynes_probability_2003; @mcelreath_statistical_2020]. 
However, if background information is incorporated via the priors then it leads to more reliable estimates [@jaynes_probability_2003; @mcelreath_statistical_2020], particularly with small sample sizes.

## P-values

In 2016 the American Statistical Association [@wasserstein_asas_2016] provided a statement on p-values. 
It included the following

**A p-value, or statistical significance, does not measure the size of an effect or the importance of a result.**

> Smaller p-values do not necessarily imply the presence of larger or more important effects, and larger p-values do not imply a lack of importance or even lack of effect. 
Any effect, no matter how tiny, can produce a small p-value if the sample size or measurement precision is high enough, and large effects may produce unimpressive p-values if the sample size is small or measurements are imprecise.


```{r sig, echo = FALSE, fig.height = 3, fig.width = 6, fig.cap = "Example 95% confidence intervals."}
library(ggplot2)
library(scales)

large <- data.frame(y = 0, ymin = -0.2, ymax = 0.2) 
small <- data.frame(y = 0, ymin = -0.05, ymax = 0.05)

data <- rbind(large, small, large - 0.06, small - 0.06, 
                  large - 0.225, small - 0.225)

data$y[1] <- -0.15
data$ymin[1] <- -0.5
data$ymax[1] <- 0.2

data$Significance <- factor("Not Significant", levels = c("Not Significant", "Significant"))
data$Significance[data$ymax < 0] <- "Significant"

data$x <- 1:nrow(data)

gp_significance <- ggplot(data = data, aes(x = x, y = y)) +
  geom_pointrange(aes(ymin = ymin, ymax = ymax, color = Significance)) +
  geom_hline(yintercept = 0, linetype = "dotted") +
  scale_y_continuous("Effect (%)", labels = percent) + 
  scale_color_manual(values = c("black", "blue")) + 
  scale_x_continuous(breaks = 1:6) +
  ylab("Estimate") +
  theme(axis.title.y = element_blank()) +
  NULL

gp_significance
```

In other words, p-values confound the estimated size of the effect with the uncertainty (width of the confidence interval) which is what they were intended to do [@greenland2019]. 
The solution is to report the estimate and associated confidence/credible intervals; an approach sometimes referred to as effect size estimation/reporting.

**Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.**

> Practices that reduce data analysis or scientific inference to mechanical 'bright-line' rules (such as 'p \< 0.05') for justifying scientific claims or conclusions can lead to erroneous beliefs and poor decision making...Pragmatic considerations often require binary, 'yes-no' decisions, but this does not mean that p-values alone can ensure that a decision is correct or incorrect.

Important decisions should be based on decision analysis, ie the estimated consequences (with uncertainty) of alternative actions on human values [@hemming_introduction_2022].
And data collection, which is itself a form of decision, should be based on the probable benefits of improving a decision versus the cost of collecting the data [@canessa2015]. 

However, less consequential data collections decisions such as how many salmonids to sample or tag can be based on simpler heuristics such as the expected power.

## Power Analysis

Power analysis can be used to determine how much data to collect to have a reasonable probability (typically 0.8) of a statistically significant result (typically p-value \< 0.05) for a given effect size.

In the case of very simple models, heuristic equations have been developed to allow the power to be quickly calculated.
More complex models require hundreds of data sets to be simulated and analysed which depending on the complexity of the model can require a lot of computational time.

## Summary

In short the use of frequentist power analyses to determine the number of salmon to tag and scale can be viewed and justified as a quick Bayesian Decision Analytic heuristic.

However, it is important to have a general understanding of the theoretical background so that if frequentist power analysis proves inadequate for a particular problem it is clear how to alter it to make an informed decision. 
For more information see McElreath [@mcelreath_statistical_2020].

<!--chapter:end:01-theory.Rmd-->

# Software

## Installation

R is open source statistical software that can be downloaded from [CRAN](https://cran.r-project.org/).

Once R has been installed the two R packages associated with this course can be installed by running the following code in the R console.
```{r, eval=FALSE}
install.packages("pak")
pak::pak("poissonconsulting/DFOsalmonids")
pak::pak("poissonconsulting/DFOsamplestats")
```

The packages can then be loaded using

```{r}
library(DFOsalmonids)
library(DFOsamplestats)
```

## Data Package

The `DFOsalmonids` data package provides two data sets both provided by the Department of Fisheries and Oceans.

The first is catch and release data.

```{r}
library(tibble)

DFOsalmonids::salmonids
```

The second scale age data.

```{r}
DFOsalmonids::scale_age
```

## Analytic Package

[`DFOsamplestats`](https://poissonconsulting.github.io/DFOsamplestats/index.html) provides functions to calculate the power (at a significance level of 0.05) using [`rate2_power()`](https://poissonconsulting.github.io/DFOsamplestats/reference/rate2_power.html) and sample size to achieve a power of 0.8 for the difference in the rate between two groups using [`rate2_samplesize()`](https://poissonconsulting.github.io/DFOsamplestats/reference/rate2_samplesize.html).

```{r}
library(DFOsamplestats)
rate2_power(p1 = 0.005, p2 = 0.0075, n = 31197)
rate2_samplesize(p1 = 0.005, p2 = 0.0075)
```

`DFOsamplestats` also provides the [`rate_effect()`](https://poissonconsulting.github.io/DFOsamplestats/reference/rate_effect.html) to estimate the effect size and p-values  based on the number of individuals returning in each group given the number tagged.

```{r}
rate_effect(r = c(156,  234), n = 31197)
```

The p-value for the first group is for $p_1 = 0.5$ while the p-value for subsequent groups is for $p_n = p_1$.

And the [`rate_sim()`](https://poissonconsulting.github.io/DFOsamplestats/reference/rate_sim.html) simulate data.

```{r}
set.seed(99)
rate_sim(p = c(0.005, 0.0075), n = 31197)
rate_sim(p = c(0.005, 0.0075), n = 31197)
```

`DFOsamplestats` also provides the [`rate2_power_analysis()`](https://poissonconsulting.github.io/DFOsamplestats/reference/rate2_power_analysis.html) function to perform a full power analysis.

```{r}
set.seed(100)
rate2_power_analysis(p1 = 0.005, p2 = 0.0075, n = 31197)
```

### Bayesian

Finally `DFOsamplestats` also provide [`rate_effect_bayesian()`](https://poissonconsulting.github.io/DFOsamplestats/reference/rate_effect_bayesian.html) and [`rate2_power_analysis_bayesian()`](https://poissonconsulting.github.io/DFOsamplestats/reference/rate2_power_analysis_bayesian.html) to estimate effect sizes and power incorporating prior information.

```{r}
set.seed(100)
rate_effect_bayesian(r = 1, n = 10, alpha = 5, beta = 5)
rate2_power_analysis(p1 = 0.005, p2 = 0.0075, n = 31197, nsims = 100)
```

### Heuristics

It is worth noting that `rate2_power()`, which is a wrapper on `stats::power.prop.test()`,
uses a heuristic to calculate the power.
As a result it is quicker but less reliable at small sample sizes than `rate2_power_analysis()` which performs a full power analysis.
However, `rate2_power_analysis()` still assumes that the likelihood profile is normally distributed which is typically not the case with little data at very low or high rates.
For the most reliable estimate use `rate2_power_analysis_bayesian()` with `nsims = 1000`.

```{r}
set.seed(100)
rate_effect(0, 5)
rate_effect_bayesian(0, 5)

rate2_power(0.1, 5)
rate2_power_analysis(0.1, 5)
rate2_power_analysis_bayesian(0.1, 5, nsims = 100)
```

<!--chapter:end:02-software.Rmd-->

# Scenarios

Before considering the scenarios it is important to load the `DFOsamplestats` package.

```{r}
library(DFOsamplestats)
```

## Fish Tagging Two Groups

Consider two groups of fish. Group 1 is standard release timing and Group 2 is late release timing.
Historic data suggests that fish in Group 1 have return rate of 0.5% while fish in Group 2 have a return rate of 0.75%.

> Question: How many fish do we need to tag in total to be able to detect this expected difference in survival?

We can answer this question using `rate2_samplesize()`.

```{r}
rate2_samplesize(p1 = 0.005, p2 = 0.0075)
```
This is the total number of fish that should be tagged (split equally between the two groups).

If we are willing to assume that any difference is positive then the sample size would be smaller.

```{r}
rate2_samplesize(p1 = 0.005, p2 = 0.0075, alternative = "greater")
```

We can confirm that we have a power of 0.8 using `rate2_power()`.

```{r}
rate2_power(p1 = 0.005, n = 24574, p2 = 0.0075, alternative = "greater")
```
As the data set is relatively big the slower but more reliable `rate2_power_analysis()` gives a very similar answer.

```{r}
set.seed(99)
rate2_power_analysis(p1 = 0.005, n = 24574, p2 = 0.0075, alternative = "greater")
```

## Fish Tagging Multiple Groups

Consider the following data set

```{r}
data <- tibble::tribble(
  ~year, ~tagged, ~catch, ~escapement,
2017, 10000, 1291,	152,
2018, 10000,	1465,	210,
2019, 10000, 898,	165,
2020, 10000,	1876,	256)
print(data)
```

> Question: Are the rates significantly different across years / which years? How do I create confidence intervals?

This can be done using `rate_effect()`. 
It is worth noting that the first p-value is the null hypothesis that $p_1 = 0.5$ while the subsequent p-values are for the null hypothesis that $p_n = p_1$.

```{r}
effect <- rate_effect(r = data$escapement, n = data$tagged)
effect$year <- data$year
print(effect)
```

And the results plotted using `ggplot2`.

```{r}
library(ggplot2)
library(scales)

ggplot(data = effect) +
  aes(x = year, y = estimate) +
  geom_pointrange(aes(ymin = lower, ymax = upper)) +
  expand_limits(y = 0) +
  scale_x_continuous("Year") +
  scale_y_continuous("Percent (%)", labels = percent)
```

> Question: If not significant, how do I figure out how many more tags I should have applied / fisheries recoveries should I have made?

```{r}
p1 <- effect$estimate[1]
p2 <- effect$estimate[3]
rate2_samplesize(p1, p2)
```

## Age Proportions Two Groups

Fish return at age 2, 3, 4, and 5. 
Age 5 fish comprise about 10% of the returns based on historic scale data we have. 

> Question: How many scales samples do we need to collect in total to be able to detect a 2% difference in the age 5 component?

```{r}
rate2_samplesize(p1 = 0.1, p2 = 0.12)
```

This is the total number of scale samples that should be taken (split evenly between the two years) if we are unwilling to assume the direction of change.

```{r}
rate2_power(p1 = 0.1, p2 = 0.12, n = 7682)
```

If we were willing to assume that any change was positive then we would have greater power.

```{r}
rate2_power(p1 = 0.1, p2 = 0.12, n = 7682, alternative = "greater")
```
  
## Age Proportions Multiple Groups

Fish return at age 2 (5%), 3 (30%), 4 (50%), and 5 (15%).

A useful way to understand the uncertainty at different sample sizes is to simulate a data set and estimate the values.

```{r}
set.seed(42)
sim <- rate_sim(p = c(0.05, 0.3, 0.5, 0.15), n = 1000)
effect <- rate_effect(r = sim$r, n = sim$n)
```

The estimated rates and associated 95% CIs can be plotted using the `ggplot2` package.

```{r}
ggplot(data = effect) +
  aes(x = group, y = estimate) +
  geom_pointrange(aes(ymin = lower, ymax = upper)) +
  expand_limits(y = 0) +
  scale_x_discrete("Group", labels = paste0("Age-", 2:5)) +
  scale_y_continuous("Percent (%)", labels = percent)
```



<!-- > Questions: How many scale (age) samples do we need to collect to confidently assert that our sample is representative of the true population age proportions? -->

<!-- Does the size of the population affect this (i.e. does the sample size different when one population sees an average return of 500 vs 10,000? -->

<!-- Can we pool age results across multiple years? Can we simply take an average? What are the considerations/caveats to this approach -->

<!-- Is the sample size smaller if there are less age classes? How do we factor this into sample size calculation? -->

<!--chapter:end:03-scenarios.Rmd-->

# General Linear Models

General linear models represent the extension of linear models such as t-tests, ANOVAs and ANCOVAs beyond the assumption of normally distributed data.

## Linear Models

Linear models are of the form

$$\mu_i = \alpha_X[X_i] + \beta_Y \cdot Y_i +...$$
and 
$$ X_i \sim N(\mu_i, \sigma) $$

which is to say that the expected value ($\mu_i$) for the ith observation ($X_i$) is the result of 1) summing particular parameter values for particular groups and/or multiplying parameter values by predictor variables to produce 2) expected values that can be positive or negative where 3) the data are normally distributed about the expected values.

## General Linear Models

General linear models extend linear models through the use of a link function to ensure the expected values are positive or bounded between 0 and 1 (typically log or log odds, respectively) and/or the use of distributions other than the normal such as the Bernoulli for binary outcomes, the Poisson for counts, the binomial for number of successful trials, the log-normal for positive continuous values etc.

For example consider the following model to estimate the expected count of fish in a pool.

$$\log(\mu_i) = \alpha_X[X_i] + \beta_Y \cdot Y_i +...$$

$$ X_i \sim \text{Poisson}(\mu_i) $$

## Transformations

There are two transformations that are important to understand when interpreting statistical analyses.

### Logarithmic Transformation

Taking the [logarithm](#logarithms) of a number between $0$ and $\infty$ rescales it between $-\infty$ and $\infty$.
In statistical modeling this relationship is used to ensure unbounded parameters provide positive estimates for response variables such as length or density.

```{r, fig.width = 6, fig.height = 4, echo = FALSE, fig.cap = "Logarithmic functions by common bases."}
library(dplyr)
library(tidyr)

data <- tibble(x = seq(0.1, 10, by = 0.001)) %>%
  mutate(`2` = log(x, base = 2),
         `2.718` = log(x),
         `10` = log(x, base = 10)) %>%
  pivot_longer(-x, names_to = "base") %>%
  mutate(base = factor(base, levels = c("2", "2.718", "10")))
  
gp <- ggplot(data = data) +
  aes(x = value, y = x, group = base, color = base) +
  geom_line() +
  scale_y_continuous(breaks = 0:10) +
  scale_x_continuous("log(x)", breaks = -4:4)

gp
```

### Logistic Transformation

The logistic transformation is the logarithm of the odds

$$\text{logit(x)} = \log\big(\frac{x}{1-x}\big)$$

Taking the log-odds of a number between $0$ and $1$ rescales it between $-\infty$ and $\infty$. 
This relationship is used to ensure unbounded parameters provide estimates between 0 and 1 for response variables such as probability of detection.

```{r, fig.width = 6, fig.height = 4, echo = FALSE, fig.cap = "Logistic transformation."}
data <- tibble(x = seq(0.01, 0.99, by = 0.01)) %>%
  mutate(y = log(x / (1 - x)))
  
gp <- ggplot(data = data) +
  aes(x = y, y = x) +
  geom_line() +
  scale_y_continuous(breaks = seq(0, 1, by = 0.1)) +
  scale_x_continuous("logit(x)", breaks = -4:4)

gp
```

## Geocentric Models

@mcelreath_statistical_2020 describes linear models and general linear models as geocentric in the model in that they can predict observational data but typically provide poor predictions of the consequences of management actions.

<iframe width="700" height="394"
src="https://www.youtube.com/embed/tNOu-SEacNU?si=RxSiy0Hm_lAmf1pX" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<!--chapter:end:04-general-linear-models.Rmd-->

# Hierarchical Models

Hierarchical models are so named because the are composed of models within models.

## Fixed Effects

In a simple fixed effects (no pooling) model like an ANOVA the expected value for each group is estimated independently of the value for the other groups in the sense that knowing something about the value for one group tells you nothing about the value(s) for the other group.

This might be represented mathematically as follows

$$ L_{i,j} \sim N(\alpha_j, \sigma') $$

where $L_{i,j}$ is the length of the ith fish in the jth group, $\sim N()$ is the normal distribution, $\alpha_i$ is the average (expected) length for the jth group and $\sigma'$ is the standard deviation of the fish within group variation.

## Random Effects

Random effects (partial pooling) allow the distribution of parameter values as opposed to just the distribution of data to be estimated through a hierarchical structure.
In this way knowing something about the value for one group in an ANOVA would tell you something about the value(s) for the other groups and vice versa.

Expanding our ANOVA to include a random effect of group would produce the following hierarchical (mixed-effects) model

$$ \alpha_j \sim N(\bar\alpha, \sigma) $$

$$ L_{i,j} \sim N(\alpha_j, \sigma) $$

where $\bar\alpha$ is the expected length for a typical group and $\sigma$ is the standard deviation of the among group variation.

## No Effects

A third alternative is the no group effects (no pooling) model 

$$ L_{i,j} \sim N(\alpha, \sigma') $$

$\alpha$ is the expected length for all fish pooled together.

In a sense a random effects models represent a balance (partial pooling) between the equivalent fixed effect model (no pooling) and no effect model (complete pooling) depending on the value of $\sigma$.

## Why Random Effects

Random effects (referred to as varying effects by @mcelreath_statistical_2020) can be used when there are at least three groups (ideally five or more) and it makes sense to think about a distribution of groups although the inclusion of random effects particularly multiple nested random effects can slow model converge.

### Pseudoreplication

Random effects can account for pseudoreplication (including overdispersion).

### Missing Data

Random effects allow us to make inferences about groups with little to no data.

### Variability

Random effects allow us to estimate the among group variation.

### Better Predictions

Random effects improve predictions by tuning the value of $\alpha'$

## Frogs in Tanks

<iframe width="700" height="394"
src="https://www.youtube.com/embed/iwVqiiXYeC4?si=dq3Z9Qjx86wci9KK&amp;start=1314" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<!--chapter:end:05-hierarchical-models.Rmd-->

# Prior Information

Bayesian analysis allows the incorporation of prior information which represents a form of regularization in the sense that biologically implausible results are not considered reasonable without adequate support in the data.

Consider a case where a biologist captures five returning salmon at random from run of 10,000 individuals and discovers that they are all males.

## Frequentist with Exact Estimate

Under a strict interpretation of the frequentist framework they should tell their manager that the sex ratio is skewed towards males (the p-value for the null hypothesis of a 50:50 sex ratio is $0.5^5 = 0.031$) and that their best estimate is that all 10,000 individuals are males!
They could also publish a paper discussing their statistically significant result.

## Frequentist with Approximate Estimate

If they were use a heuristic to calculate an approximate estimate they would report that the best estimate is that all the fish are males although they wouldn't be able to publish a paper as they wouldn't be able to rule out the possibility that all the fish are females!

```{r}
library(DFOsamplestats)
rate_effect(0, 5)
```

### Bayesian with Uninformative Prior

If they were fit the equivalent Bayesian model then even with an uninformative prior (beta distribution with an alpha of 1 and a beta of 1) they would still conclude that the sex ratio was significantly different from 50:50 but their estimate (if using the median value of the posterior probability) would be that approximately 10% of the run was female.

```{r}
plot_dbeta()
set.seed(42)
rate_effect_bayesian(0, 5)
```

### Bayesian with Weakly Informative Prior

If they were to fit the Bayesian model with a weakly informative prior say a beta distribution with alpha = 11 and beta = 11 then they would estimate that about 40% of the run was female although they would consider in reasonably likely that females make up 60% of the run.

```{r}
plot_dbeta(11, 11)
set.seed(42)
rate_effect_bayesian(0, 5, alpha = 11, beta = 11)
```

### Bayesian with Informative Prior

With an informative prior then they would conclude that knowing the sex of five fish barely changes the uncertainty around the sex ratio which based on natural selection and previous experience is likely very close to 50:50.

```{r}
set.seed(42)
rate_effect_bayesian(0, 5, alpha = 101, beta = 101)
```

Indeed given this much prior information then they would require a sample size of ~30 males to produce a pvalue < 0.05.

```{r}
set.seed(42)
rate_effect_bayesian(0, 30, alpha = 101, beta = 101)
```


<!--chapter:end:06-prior-information.Rmd-->

# Case Studies

## Age Proportions

The following code estimates the proportion of each age class by brood year with 95% CIs.

The warning and confidence intervals that include 0 for zero counts are because the code uses the Wald heuristic.

```{r, fig.width = 6, fig.height=8, fig.cap="Estimated percent composition by age and brood year."}
library(dplyr)
data <- DFOsalmonids::scale_age

data %<>%
  group_by(brood_year) %>%
  mutate(total_captures = sum(captures)) %>%
  ungroup()

effect <- rate_effect(data$captures, data$total_captures)

data <- bind_cols(data, effect)

gp <- data %>%
  ggplot() +
  aes(x = age, y = estimate) +
  facet_wrap(~brood_year) +
  geom_pointrange(aes(ymin = lower, ymax = upper)) +
  scale_x_continuous("Age") +
  scale_y_continuous("Proportion (%)", labels = percent) +
  NULL

print(gp)
```

Based on visual examination of the plots it is clear that there are substantial differences among years.

<!--chapter:end:07-case-studies.Rmd-->

# Distributions

A handful of probability distributions can be used to implement a surprising number of different models.

## Continuous Distributions

Continuous distributions describe the probability density for any given value within a particular range.

### Uniform Distribution

As its name suggests the [uniform distribution](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)) has a constant density between a lower and upper bound. The uniform distribution can be useful for defining an uninformative prior.

```{r, fig.cap = "A uniform distribution with a lower bound of 0 and an upper bound of 1."}
data <- data.frame(x = seq(-0.1, 1.1, length.out = 1000))
data$y <- dunif(data$x)
gp <- ggplot(data, aes(x = x, y = y)) +
  geom_line() +
  expand_limits(y = 0) +
  scale_y_continuous("Density")
gp + scale_x_continuous(breaks = c(0, 0.5, 1))
```

### Normal Distributions

Normal distributions are the most common type of distribution.

#### Normal Distribution

A [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) is a bell-shaped curve.

```{r, fig.cap = "A (standard) normal distribution with a mean of 0 and a standard deviation of 1."}
data <- data.frame(x = seq(-3, 3, length.out = 50))
data$y <- dnorm(data$x)

gp %+% data
```

#### Log-Normal Distribution

The [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution) is a normal distribution on a logarithmic scale which means that all the possible values are positive. It is useful for modeling sizes or densities of organisms.

```{r, fig.cap = "A log-normal distribution with a mean on the log scale of 0 and a standard deviation on the log scale of 1"}
data <- data.frame(x = seq(0, 5, length.out = 100))
data$y <- dlnorm(data$x)

gp %+% data
```

### Beta Distribution

The [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) is a continous distribution on the probability scale which means that all the possible values are between 0 and 1.

```{r, fig.cap = "A beta distribution with an alpha (shape1) parameter of 5 and a beta (shape2) parameter of 2."}
data <- data.frame(x = seq(0, 1, length.out = 100))
data$y <- dbeta(data$x, 5, 2)

gp %+% data
```

## Discrete Distributions

Discrete distributions describe the probability for each of the possible outcomes.

#### Bernoulli Distribution

The [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) gives the probabilities for binary outcomes such tossing a coin or whether an individual survives.

```{r, fig.cap="A Bernoulli distribution where the probability of outcome 1 is 0.7"}
data <- data.frame(x = 0:1)
data$y <- dbinom(data$x, 1, 0.7)

gp <- ggplot(data, aes(x = x, y = y)) +
  geom_col() +
  expand_limits(y = 0) +
  scale_y_continuous("Probability")

gp + scale_x_continuous(breaks = 0:10)
```

### Binomial Distribution

The [binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution) is an generalization of the Bernoulli distribution to multiple trials such the number of heads when tossing five coins or the number of marked fish that are resighted.
It is useful for modeling survival or mark-recapture data.

```{r, fig.cap="A binomial distribution for five trials with a probability of success of 0.7."}
data <- data.frame(x = 0:5)
data$y <- dbinom(data$x, 5, 0.7)
gp %+% data + scale_x_continuous(breaks = 0:5)
```

### Poisson Distribution

The [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution) gives the probabilities for a number of independent events such as observing an individual during a survey.
It is useful for analysing count data.

```{r, fig.cap="A Poisson distribution where the mean number of events is 3."}
data <- data.frame(x = 0:10)
data$y <- dpois(data$x, 3)
gp %+% data + scale_x_continuous(breaks = 0:10)
```

<!--chapter:end:08-distributions.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:10-references.Rmd-->

