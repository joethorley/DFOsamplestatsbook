# Case Studies

## Age Proportions

The following code estimates the proportion of each age class by brood year with 95% CIs.

The warning and confidence intervals that include 0 for zero counts are because the code uses the Wald heuristic.

```{r, fig.width = 6, fig.height=8, fig.cap="Estimated percent composition by age and brood year."}
library(dplyr)
data <- DFOsalmonids::scale_age

data %<>%
  group_by(brood_year) %>%
  mutate(total_captures = sum(captures)) %>%
  ungroup()

effect <- rate_effect(data$captures, data$total_captures)

effect <- bind_cols(data, effect)

gp <- effect %>%
  ggplot() +
  aes(x = age, y = estimate) +
  facet_wrap(~brood_year) +
  geom_pointrange(aes(ymin = lower, ymax = upper)) +
  scale_x_continuous("Age") +
  scale_y_continuous("Proportion (%)", labels = percent) +
  NULL

print(gp)
```

A Bayesian model correctly handles the 0 counts.

```{r, fig.width = 6, fig.height=8, fig.cap="Estimated percent composition by age and brood year."}
library(dplyr)

effect <- rate_effect_bayesian(data$captures, data$total_captures)

effect <- bind_cols(data, effect)

gp <- effect %>%
  ggplot() +
  aes(x = age, y = estimate) +
  facet_wrap(~brood_year) +
  geom_pointrange(aes(ymin = lower, ymax = upper)) +
  scale_x_continuous("Age") +
  scale_y_continuous("Proportion (%)", labels = percent) +
  NULL

print(gp)
```

Based on visual examination of the plots it is clear that there are substantial differences among years.

Rather than fitting a multinomial a more informative approach would be to estimate marine survival and percent return strategies.

## Smolt vs Seapen Survival Rate

```{r, echo = FALSE}
data <- tribble(
~Brood_Year, ~Tagcode, ~Tags,	~Release_Stage,	~Survival_Rate,
2013,	183670,	59988,	"Smolt 0+",	0.002038574,
2013,	183671,	59802,	"Seapen 0+",	0.007729173,
2014,	182694,	27712,	"Smolt 0+",	0.010796767,
2014,	182695,	31584,	"Seapen 0+",	0.013691109,
2014,	182696,	29104,	"Smolt 0+",	0.006679838,
2014,	182697,	25677,	"Seapen 0+",	0.018492425,
2015,	182866,	27205,	"Seapen 0+",	0.007199779,
2015,	182867,	25245,	"Seapen 0+",	0.009332541,
2015,	182868,	23202,	"Smolt 0+",	0.009084562,
2015,	182869,	4393, "Smolt 0+",	0.048893695,
2016,	184165,	29044,	"Seapen 0+",	0.011012946,
2016,	184166,	28626,	"Seapen 0+",	0.013881436,
2016,	184167,	30238,	"Smolt 0+",	0.00817349,
2016,	184168,	28922,	"Smolt 0+",	0.006823525,
2017,	185767,	28979,	"Seapen 0+",	0.011607716,
2017,	185768,	27324,	"Seapen 0+",	0.012670546,
2017,	185769,	31557,	"Smolt 0+",	0.002034731,
2017,	185770,	28216,	"Smolt 0+",	0.001786575,
2018,	185779,	29680,	"Seapen 0+",	0.014877022,
2018,	185780,	26753,	"Seapen 0+",	0.012624005,
2018,	185781,	28326,	"Smolt 0+",	0.004970698,
2018,	185782,	26733,	"Smolt 0+",	0.005236973)
```

Consider the following data set.

```{r,message=FALSE}
data <- mutate(data, Returns = round(Tags * Survival_Rate))
print(data)
```

And the following prior for the return rate.
```{r}
plot_dbeta(alpha = 1, beta = 11)
```

A Bayesian model estimates that survival for smolt 0+ is highly variable among years. It also indicates that there are meaningful differences among TagCodes.

```{r}
effect <- rate_effect_bayesian(r = data$Returns, n = data$Tags,
                               alpha = 1, beta = 11)

effect <- bind_cols(data, effect)

gp <- effect %>%
  ggplot() +
  aes(x = Brood_Year, y = estimate) +
  geom_pointrange(aes(ymin = lower, ymax = upper, color = Release_Stage, shape = Release_Stage), position = position_dodge(width = 0.25)) +
  scale_x_continuous("Brood Year") +
  scale_y_continuous("Survival (%)", labels = percent) +
  NULL

print(gp)
```

In this situation I would consider a model with a random effect of year within release stage with different annual variability for each stage.
I would also consider including a random effect of tag code. 
This model is still geocentric in that it doesn't allow us to predict the effect of a management action.

## Chehalis Tubs

Three populations in separate tubs (T1, T2, T3) swam through plumbing and may have mixed between tubs.  
It is unknown the direction that the fish moved and we only know that tub T2 has more fish than it did previously and T3 has less.  
We want to find out if any of the tubs are single stock and the rough proportion of stocks in mixed tubs?

Assumptions 
  - Movement between tubs has stopped 
	- Fish are randomly selected from tubs for sampling
	- Fish are not sampled twice 

Based on this very limited information I would plot how the uncertainty in the proportion of a population that is one stock varies with the sample size with uninformative priors.

```{r}
estimate <- rate_effect_bayesian(r = 0:100, n = 0:100)

print(tail(estimate))

gp <- estimate %>%
  ggplot() +
  aes(x = n, y = estimate) +
  geom_line() +
  geom_line(aes(y = lower), linetype = "dotted") +
  geom_line(aes(y = upper), linetype = "dotted") +
  scale_x_continuous("Sample Size") +
  scale_y_continuous("Proportion (%)", labels = percent) +
  NULL

print(gp)
```

To make the relationship clearer I would plot it on the log odds (logistic) scale.

```{r}
gp <- estimate %>%
  ggplot() +
  aes(x = n, y = estimate) +
  geom_line() +
  geom_line(aes(y = lower), linetype = "dotted") +
  geom_line(aes(y = upper), linetype = "dotted") +
  scale_x_continuous("Sample Size") +
  scale_y_continuous("Proportion (%)", transform = "logit", labels = percent, breaks = c(0.01, 0.1,0.5,0.9, 0.95, 0.99, 0.999, 0.9999)) +
  NULL

print(gp)
```

For the question of single stocks I may settle on 75 fish from each tank because if one individual was a different stock it would indicate it was a mixed stock but if all were the same stock then it would allow us to estimate that there is a 95% that the stock is more than 95% pure.

For the other question I might simply plot how the required sample size to detect a significant difference from 0.5 with a power of 0.8 varies by difference.

```{r}
data <- tibble::tibble(p = seq(0, 0.49, by = 0.01))
data$n <- purrr::map_int(data$p, ~rate2_samplesize(.x))

gp <- data %>%
  ggplot() +
  aes(x = p, y = n) +
  geom_line() +
  scale_x_continuous("Proportion (%)", labels = percent) +
  scale_y_continuous("Sample Size", labels = comma) +
  NULL

print(gp)
```
I would likely use the log transform to better understand how the sample size requirement varies.

```{r}
gp <- data %>%
  ggplot() +
  aes(x = p, y = n) +
  geom_line() +
  scale_x_continuous("Proportion (%)", labels = percent) +
  scale_y_log10("Sample Size", labels = comma) +
  NULL

print(gp)
```

I would then ask what current or future management decisions might be altered by reducing the uncertainty in the proportional of each tub and what the likely benefits of an improved decision might be and I would ask what the cost of sample fish is.

At the extremes if no decision would be altered I wouldn't sample any fish and if no cost to sampling I would sample all the fish!

If I was blind to the potential costs and benefits I would probably settle for a sample size of 1,000 based on an argument of diminishing returns.

If I would concerned about the validity of the heuristic I would then validate this result based on a Bayesian power analysis with `nsims = 1000`.

```{r}
set.seed(42)
rate2_power_analysis_bayesian(p1 = 0.45, n = 1000, nsims = 100)
```
